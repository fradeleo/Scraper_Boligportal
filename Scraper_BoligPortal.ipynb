{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoligPortal Scraper (with Selenium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProgressBar Function to follow the scraper's progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import printProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium start-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< Insert your starting URL in the line below >>>\n",
    "\n",
    "# First page of ads in Kbh Kommune and Frederiksberg Kommune\n",
    "start_url='https://www.boligportal.dk/find?placeIds=15%2C365&minRentalPeriod=2' \n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/Francesco/Documents/Learn/chromedriver\", options=options)\n",
    "driver.get(start_url)\n",
    "sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for the .csv results\n",
    "\n",
    "wd = \"/Users/Francesco/Documents/Learn/Other/Franz/Boligportal\"\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%d%m%y\")\n",
    "dirName = current_time\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(os.path.join(wd, dirName))\n",
    "    print(\"Directory \" , dirName ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dirName ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the number of pages from the last Pagination button at the bottom of the page\n",
    "\n",
    "soup = bs(driver.page_source, 'html')\n",
    "num_pages = soup.find_all('a', {'class': 'PaginationControls__page'})[-1].getText() \n",
    "num_pages = int(num_pages)\n",
    "\n",
    "# Extract number of ads\n",
    "num_ads = num_pages * 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of numbers to generate the URLs to scrape\n",
    "start_record = np.arange(0, num_ads, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate URLs to scrape, corresponding to page numbers (1, 2, 3, etc..)\n",
    "\n",
    "urls = []\n",
    "\n",
    "for i in start_record:\n",
    "    url = start_url + '&startRecord={}'.format(i)\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Selenium will go through each page (from the URLs list) and extract the AdCardWrappers\n",
    "# Then, it will extract each AdCard's information\n",
    "\n",
    "data = []\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    driver.get(url)\n",
    "    sleep(1)\n",
    "    soup = bs(driver.page_source, 'html')\n",
    "    \n",
    "    ads_buttons = soup.find_all('div', {'class': 'AdCardWrapper'})\n",
    "    \n",
    "    for ad in ads_buttons:\n",
    "    \n",
    "        try:\n",
    "            title = ad.find('div', {'class': 'AdCard__title'}).getText() \n",
    "        except:\n",
    "            title = -1\n",
    "\n",
    "        try:\n",
    "            price = ad.find('div', {'class': 'AdCard__price'}).getText() \n",
    "        except:\n",
    "            price = -1\n",
    "\n",
    "        try:\n",
    "            location = ad.find('div', {'class': 'AdCard__location'}).getText() \n",
    "        except:\n",
    "            location = -1\n",
    "\n",
    "        try:\n",
    "            date = ad.find('div', {'class': 'AdCard__date'}).getText() \n",
    "        except:\n",
    "            date = -1\n",
    "\n",
    "        try:\n",
    "            link = 'boligportal.dk' + ad.find('a', {'itemprop' : 'url'}).get('href')\n",
    "        except:\n",
    "            link = -1\n",
    "\n",
    "        try:\n",
    "            description = ad.find('div', {'class': 'AdCard__description'}).getText() \n",
    "        except:\n",
    "            description = -1\n",
    "\n",
    "\n",
    "        data.append({\"Title\": title,\n",
    "                     \"Price\": price,\n",
    "                     \"Location\": location,\n",
    "                     \"When\": date,\n",
    "                     \"Description\": description,\n",
    "                     \"URL\": link})\n",
    "\n",
    "    printProgressBar(urls.index(url), len(urls), prefix = 'Scraping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped data preview\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with the data\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(subset=None, keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to clean/manipulate the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rooms(row):\n",
    "    \n",
    "    ''' Function to use with df.apply. Creates a Rooms column by extracting the number of rooms from the Ad title '''\n",
    "    \n",
    "    if 'Værelse' in row.Title:\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            if len([int(s) for s in row.Title.split() if s.isdigit()]) == 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return [int(s) for s in row.Title.split() if s.isdigit()][0]\n",
    "        except:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m2(row):\n",
    "    \n",
    "    ''' Function to use with df.apply. Creates a m2 column by extracting the squared meters from the Ad title '''\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            return [int(s) for s in row.Title.split() if s.isdigit()][1]\n",
    "        except:\n",
    "            return [int(s) for s in row.Title.split() if s.isdigit()][0]\n",
    "    \n",
    "    except: \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_et_to_one(column):\n",
    "    \n",
    "    '''To use on the column containing the age of the Ad. Replaces 'en and 'et' with 1, Fremhævet with 0, removes 'siden' '''\n",
    "    \n",
    "    column = column.str.replace('Fremhævet', '0')\n",
    "    column = column.str.replace('siden', '')\n",
    "    column = column.str.replace('en','1', 1)\n",
    "    column = column.str.replace('et','1', 1)\n",
    "    \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posted_x_days_ago(row):\n",
    "    \n",
    "    ''' Extracts age of post in terms of days '''\n",
    "\n",
    "    if 'dag' in row.When:\n",
    "        return [int(s) for s in row.When.split() if s.isdigit()][0]\n",
    "    \n",
    "    elif 'måned' in row.When:\n",
    "        return [int(s) for s in row.When.split() if s.isdigit()][0] * 30\n",
    "    \n",
    "    elif 'år' in row.When:\n",
    "        return [int(s) for s in row.When.split() if s.isdigit()][0] * 365\n",
    "\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe cleaning/manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rooms'] = df.apply(lambda row: get_rooms(row), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m2'] = df.apply(lambda row: get_m2(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price'] = df['Price'].apply(lambda row: row.replace(\",-\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['Neighbourhood','Street']] = df.Location.str.split(\", \", expand=True)\n",
    "#df = df.drop('Empty', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.When = en_et_to_one(df.When)\n",
    "df['Posted days ago'] = df.apply(lambda row: posted_x_days_ago(row), axis = 1)\n",
    "df = df.drop('When', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Title', 'Price', 'Rooms', 'm2', 'Neighbourhood', 'Street', 'Posted days ago', 'Description', 'URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = 'Posted days ago', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(wd, dirName,'BoligPortal_KPH_{}.csv'.format(datetime.now().strftime(\"%d%m%y\"))), index = True, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
